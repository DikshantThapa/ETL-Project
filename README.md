# ETL to Insights: Employee Data Pipeline

**Author:** Dikshant Bikram Thapa  
**Date:** January 6, 2026  
**University:** Kathmandu University, CSE Final Year  
**Course:** Data Engineering / Advanced Databases  

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [Installation & Setup](#installation--setup)
4. [Project Structure](#project-structure)
5. [Running the Pipeline](#running-the-pipeline)
6. [Data Layers](#data-layers)
7. [KPI Definitions](#kpi-definitions)
8. [API Documentation](#api-documentation)
9. [Visualizations](#visualizations)
10. [Results & Analysis](#results--analysis)

---

## ğŸ“Œ Project Overview

This project implements a **complete ETL (Extract, Transform, Load) pipeline** for employee and timesheet data analytics. The pipeline processes raw CSV data, applies business logic transformations, and generates **9 key performance indicators (KPIs)** for HR analytics.

### Problem Statement
Healthcare organizations need real-time insights into employee performance, attendance patterns, and workforce trends. Manual data analysis is time-consuming and error-prone.

### Solution
An automated ETL pipeline that:
- âœ… Extracts data from multiple CSV sources
- âœ… Cleans and validates data in BRONZE layer
- âœ… Transforms data with business logic in SILVER layer
- âœ… Generates KPIs in GOLD layer
- âœ… Exposes data via REST API and interactive visualizations

---

## ğŸ—ï¸ Architecture

### Lambda/Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DATA SOURCES                  â”‚
â”‚  â”œâ”€ employee_202510161125.csv           â”‚
â”‚  â””â”€ timesheet_202509151540.csv (Ã—3)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  EXTRACT LAYER  â”‚
        â”‚ (Raw CSV Files) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  BRONZE LAYER (Raw Data)  â”‚
    â”‚ â”œâ”€ bronze_employees       â”‚
    â”‚ â””â”€ bronze_timesheets      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  SILVER LAYER (Clean Data)   â”‚
  â”‚ â”œâ”€ silver_employees          â”‚
  â”‚ â”œâ”€ silver_timesheets         â”‚
  â”‚ â”œâ”€ Data validation & cleaningâ”‚
  â”‚ â””â”€ Derived columns added     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   GOLD LAYER (KPIs)  â”‚
    â”‚ â”œâ”€ Active Headcount  â”‚
    â”‚ â”œâ”€ Turnover Trend    â”‚
    â”‚ â”œâ”€ Tenure Analysis   â”‚
    â”‚ â”œâ”€ Late Arrivals     â”‚
    â”‚ â”œâ”€ Overtime          â”‚
    â”‚ â”œâ”€ Attrition Risk    â”‚
    â”‚ â””â”€ (6 more KPIs)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼             â–¼          â–¼
   [API]      [Charts]     [Reports]
```

---

## ğŸ’» Installation & Setup

### Step 1: Clone Repository
```bash
cd ~/Desktop
git clone <repo-url>
cd data-onboarding-etl
```

### Step 2: Create Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate  # On macOS/Linux
# OR
venv\Scripts\activate  # On Windows
```

### Step 3: Install Dependencies
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Step 4: Create Environment Configuration
```bash
cat > .env << 'EOF'
DB_PATH=data/etl.db
LOG_LEVEL=INFO
BATCH_SIZE=1000
API_HOST=0.0.0.0
API_PORT=8000
JWT_SECRET=your-secret-key-change-in-production
EOF
```

### Step 5: Prepare Data Directory
```bash
mkdir -p data/raw data/bronze data/silver data/gold
# Copy CSV files to data/raw/
cp /path/to/employee_*.csv data/raw/
cp /path/to/timesheet_*.csv data/raw/
```

### Step 6: Verify Setup
```bash
python3 << 'EOF'
import sys
print(f"âœ“ Python: {sys.version}")
import pandas as pd
print(f"âœ“ Pandas: {pd.__version__}")
import duckdb
print(f"âœ“ DuckDB: {duckdb.__version__}")
import plotly
print(f"âœ“ Plotly: {plotly.__version__}")
import fastapi
print(f"âœ“ FastAPI: {fastapi.__version__}")
print("\nâœ… All dependencies installed!")
EOF
```

---

## ğŸ“‚ Project Structure

```
data-onboarding-etl/
â”‚
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # Configuration (DO NOT COMMIT)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Input CSVs
â”‚   â”‚   â”œâ”€â”€ employee_202510161125.csv
â”‚   â”‚   â”œâ”€â”€ timesheet_202509151540.csv
â”‚   â”‚   â”œâ”€â”€ timesheet_202510161121.csv
â”‚   â”‚   â”œâ”€â”€ timesheet_202510161122.csv
â”‚   â”‚   â””â”€â”€ timesheet_202510161124.csv
â”‚   â”œâ”€â”€ etl.db                   # DuckDB database (auto-created)
â”‚   â”œâ”€â”€ bronze/                  # Raw data tables
â”‚   â”œâ”€â”€ silver/                  # Cleaned data tables
â”‚   â””â”€â”€ gold/                    # KPI tables
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”‚   â””â”€â”€ flows.py             # Main ETL pipeline
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI endpoints
â”‚   â””â”€â”€ viz/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ charts.py            # Plotly visualizations
â”‚
â”œâ”€â”€ reports/                     # Generated HTML charts
â”‚   â”œâ”€â”€ 01_active_headcount.html
â”‚   â”œâ”€â”€ 02_turnover_trend.html
â”‚   â”œâ”€â”€ 03_tenure_by_dept.html
â”‚   â”œâ”€â”€ 04_late_arrivals.html
â”‚   â”œâ”€â”€ 05_overtime.html
â”‚   â””â”€â”€ 06_attrition_type.html
â”‚
â””â”€â”€ venv/                        # Python virtual environment
```

---

## ğŸš€ Running the Pipeline

### Step 1: Activate Virtual Environment
```bash
source venv/bin/activate
```

### Step 2: Run ETL Pipeline
```bash
python -m src.etl.flows
```

**Expected Output:**
```
INFO:__main__:Connected to DuckDB: data/etl.db
INFO:__main__:ğŸš€ Starting ETL Pipeline...
INFO:__main__:ğŸ“¥ Extracting employees...
INFO:__main__:âœ“ Extracted 50 employee records
INFO:__main__:ğŸ“¥ Extracting timesheets...
INFO:__main__:Found 4 timesheet files
INFO:__main__:âœ“ Extracted 412571 timesheet records
INFO:__main__:ğŸ“¤ Loading to BRONZE layer...
INFO:__main__:âœ“ Loaded 50 to bronze_employees
INFO:__main__:âœ“ Loaded 412571 to bronze_timesheets
INFO:__main__:ğŸ”„ Transforming employees...
INFO:__main__:âœ“ Transformed 50 employees
INFO:__main__:ğŸ”„ Transforming timesheets...
INFO:__main__:âœ“ Transformed 381732 timesheet records
INFO:__main__:ğŸ“¤ Loading to SILVER layer...
INFO:__main__:âœ“ Loaded 50 to silver_employees
INFO:__main__:âœ“ Loaded 381732 to silver_timesheets
INFO:__main__:ğŸ† Generating KPIs...
âœ“ kpi_active_headcount: 7 rows
âœ“ kpi_turnover_trend: 5 rows
âœ“ kpi_avg_tenure: 38 rows
âœ“ kpi_avg_working_hours: 1000 rows
âœ“ kpi_late_arrivals: 100 rows
âœ“ kpi_early_departures: 100 rows
âœ“ kpi_overtime: 100 rows
âœ“ kpi_rolling_avg: 5000 rows
âœ“ kpi_early_attrition: 2 rows
INFO:__main__:âœ… ETL Pipeline completed successfully!
```

### Step 3: Verify Data in Database
```bash
python3 << 'EOF'
import duckdb

conn = duckdb.connect('data/etl.db')

# List all tables
tables = conn.execute("SELECT table_name FROM information_schema.tables ORDER BY table_name").fetchall()
print(f"\nâœ… {len(tables)} Tables Created:\n")
for t in tables:
    count = conn.execute(f"SELECT COUNT(*) FROM {t[0]}").fetchall()[0][0]
    print(f"  {t[0]:30s} {count:>10,} rows")

conn.close()
EOF
```

---

## ğŸ“Š Data Layers

### 1ï¸âƒ£ BRONZE Layer (Raw Data)

**Purpose:** Store raw, unmodified data directly from source systems.

**Tables:**
- `bronze_employees` (50 rows)
  - Direct copy from `employee_202510161125.csv`
  - No transformations applied
  
- `bronze_timesheets` (412,571 rows)
  - Concatenation of 4 timesheet CSVs
  - Column stripping applied

**Quality Checks:**
```sql
SELECT COUNT(*) FROM bronze_employees;     -- 50
SELECT COUNT(*) FROM bronze_timesheets;    -- 412,571
```

### 2ï¸âƒ£ SILVER Layer (Clean Data)

**Purpose:** Store cleaned, validated, business-ready data.

**Transformations Applied:**

**Employee Data:**
```python
- Remove duplicates (on client_employee_id)
- Parse dates: hire_date, term_date, dob
- Add is_active flag (1 = currently employed)
- Calculate tenure_days (days since hire)
```

**Timesheet Data:**
```python
- Remove duplicates (on client_employee_id + punch_in_datetime)
- Parse all datetime columns
- Calculate minutes_late = punch_in - scheduled_start
- Calculate minutes_early = scheduled_end - punch_out
- Flag anomalies:
  * is_late = minutes_late > 5
  * is_early = minutes_early > 5
  * is_overtime = hours_worked > 8.5
  * is_normal_work = 7.5 â‰¤ hours_worked â‰¤ 8.5
```

**Quality Metrics:**
- Rows before deduping: 412,571
- Rows after deduping: 381,732
- Duplicate rate: 7.5%

### 3ï¸âƒ£ GOLD Layer (KPIs)

**Purpose:** Aggregate business metrics for reporting and analysis.

**9 KPI Tables Generated** (see next section)

---

## ğŸ“ˆ KPI Definitions

### 1. `kpi_active_headcount`
**Definition:** Count of employees actively employed each month

**Query:**
```sql
SELECT 
    DATE_TRUNC('month', punch_apply_date)::date AS month,
    COUNT(DISTINCT CASE 
        WHEN e.hire_date <= ts.punch_apply_date 
        AND (e.term_date IS NULL OR e.term_date > ts.punch_apply_date)
        THEN e.client_employee_id 
    END) AS active_headcount
FROM silver_timesheets ts
CROSS JOIN (SELECT DISTINCT client_employee_id, hire_date, term_date FROM silver_employees) e
GROUP BY DATE_TRUNC('month', punch_apply_date)
ORDER BY month DESC
```

**Sample Output:**
| month | active_headcount |
|-------|------------------|
| 2025-10-01 | 44 |
| 2025-09-01 | 44 |
| 2025-08-01 | 44 |

---

### 2. `kpi_turnover_trend`
**Definition:** Number of employees terminated each month

**Insight:** Tracks workforce attrition and churn rate

**Sample Output:**
| month | terminations |
|-------|--------------|
| 2025-06-01 | 1 |
| 2025-05-01 | 1 |

---

### 3. `kpi_avg_tenure`
**Definition:** Average tenure (years) by department

**Insight:** Identifies departments with experienced vs. new staff

**Sample Output:**
| department_name | avg_tenure_years | employee_count |
|---|---|---|
| 209037-Clinical Manager | 35.15 | 1 |
| 227001-Staff Education Admin | 33.5 | 2 |

---

### 4. `kpi_avg_working_hours`
**Definition:** Weekly average working hours per employee (normal work days only)

**Insight:** Tracks standard work hours patterns

---

### 5. `kpi_late_arrivals`
**Definition:** Count and average lateness (minutes) for employees with >5 min late punches

**Insight:** Identifies attendance issues

**Sample Output:**
| client_employee_id | late_count | avg_minutes_late |
|---|---|---|
| EMP001 | 12 | 8.5 |

---

### 6. `kpi_early_departures`
**Definition:** Count and average early departures (>5 min early) per employee

---

### 7. `kpi_overtime`
**Definition:** Employees working >8.5 hours per day

**Insight:** Tracks workload and overtime patterns

---

### 8. `kpi_rolling_avg`
**Definition:** 30-day rolling average of hours worked per employee

**Insight:** Smooths out daily variations to show trends

---

### 9. `kpi_early_attrition`
**Definition:** Employees who left within 90 days of hire vs. long-term attrition

**Insight:** Identifies onboarding issues

**Sample Output:**
| attrition_type | count |
|---|---|
| Early Attrition (<90 days) | 2 |
| Other Attrition (â‰¥90 days) | 4 |

---

## ğŸŒ API Documentation

### Starting the API Server

```bash
source venv/bin/activate
python -m uvicorn src.api.main:app --reload
```

**Server runs on:** `http://localhost:8000`

### Available Endpoints

#### 1. Health Check
```bash
GET /health

curl http://localhost:8000/health

# Response:
{
  "status": "healthy",
  "database": "data/etl.db"
}
```

#### 2. Active Headcount
```bash
GET /kpis/active-headcount

curl http://localhost:8000/kpis/active-headcount

# Returns: Monthly active employee counts
```

#### 3. Turnover Trend
```bash
GET /kpis/turnover

curl http://localhost:8000/kpis/turnover

# Returns: Monthly termination counts
```

#### 4. Tenure by Department
```bash
GET /kpis/tenure

curl http://localhost:8000/kpis/tenure

# Returns: Average tenure by department
```

#### 5. Late Arrivals
```bash
GET /kpis/late-arrivals

curl http://localhost:8000/kpis/late-arrivals

# Returns: Top 20 employees with late arrivals
```

#### 6. Overtime Data
```bash
GET /kpis/overtime

curl http://localhost:8000/kpis/overtime

# Returns: Top 20 employees with overtime
```

#### 7. Attrition Analysis
```bash
GET /kpis/attrition

curl http://localhost:8000/kpis/attrition

# Returns: Early vs. long-term attrition breakdown
```

### Interactive API Documentation

**Visit:** `http://localhost:8000/docs`

This provides:
- ğŸ“‹ Swagger UI with all endpoints
- ğŸ¯ "Try it out" button to test endpoints
- ğŸ“Š Auto-formatted JSON responses
- ğŸ“š Full API documentation

---

## ğŸ“Š Visualizations

### Generating Charts

```bash
python src/viz/charts.py
```

**Output:** 6 interactive HTML charts in `reports/` folder

### Available Charts

#### 1. Active Headcount Over Time
**File:** `reports/01_active_headcount.html`
- **Type:** Line chart with markers
- **Shows:** Headcount trend by month
- **Insight:** Workforce size evolution

#### 2. Monthly Turnover Trend
**File:** `reports/02_turnover_trend.html`
- **Type:** Bar chart
- **Shows:** Terminations per month
- **Insight:** Attrition patterns

#### 3. Average Tenure by Department
**File:** `reports/03_tenure_by_dept.html`
- **Type:** Bar chart with color scale
- **Shows:** Avg tenure and employee count by department
- **Insight:** Department experience levels

#### 4. Top Late Arrivals
**File:** `reports/04_late_arrivals.html`
- **Type:** Bar chart
- **Shows:** Top 20 employees with most late arrivals
- **Insight:** Attendance issues

#### 5. Overtime Analysis
**File:** `reports/05_overtime.html`
- **Type:** Bar chart
- **Shows:** Top 20 employees with most overtime hours
- **Insight:** Workload distribution

#### 6. Attrition Type Distribution
**File:** `reports/06_attrition_type.html`
- **Type:** Pie chart
- **Shows:** Early attrition (<90 days) vs. long-term
- **Insight:** Onboarding success rate

### Viewing Charts

```bash
# Open in browser
open reports/01_active_headcount.html
open reports/02_turnover_trend.html
# ... etc
```

---

## ğŸ“Š Results & Analysis

### Data Summary

| Metric | Value |
|--------|-------|
| Total Employees | 50 |
| Total Timesheet Records | 412,571 |
| After Deduplication | 381,732 |
| Duplicate Rate | 7.5% |
| Date Range | Sept 2025 - Oct 2025 |
| Number of Departments | 38 |
| Terminated Employees | 6 |
| Early Attrition (<90 days) | 2 |

### Key Findings

#### 1. Headcount Stability
- **Current headcount:** 44 active employees (Oct 2025)
- **Trend:** Stable across last 7 months (44-46 employees)
- **Implication:** Low turnover rate

#### 2. Tenure Profile
- **Longest tenure:** 35.15 years (Clinical Manager)
- **Average tenure:** ~27 years
- **Implication:** Highly experienced workforce

#### 3. Attrition Pattern
- **Total terminations:** 6 employees in 10-month period
- **Early attrition:** 2 employees (33%)
- **Long-term attrition:** 4 employees (67%)
- **Implication:** Onboarding needs improvement

#### 4. Attendance Issues
- **Employees with late arrivals:** 100 (tracked)
- **Max late arrivals:** One employee with 12+ late punches
- **Implication:** Small percentage with attendance issues

#### 5. Overtime
- **Employees working overtime:** 100 tracked
- **Avg extra hours:** Varies by employee
- **Implication:** Some departments may be understaffed

---

## ğŸ”„ ETL Pipeline Code Overview

### Main ETL Flow (`src/etl/flows.py`)

```python
class ETLPipeline:
    def extract_employees(self):
        # Read CSV with pipe delimiter
        # Strip whitespace
        # Return dataframe
    
    def extract_timesheets(self):
        # Read 4 CSV files
        # Concatenate into single dataframe
        # Return combined dataframe
    
    def load_bronze(self, emp_df, ts_df):
        # Register dataframes as temporary views
        # Create BRONZE tables
        # Unregister views
    
    def transform_employees(self, emp_df):
        # Remove duplicates
        # Parse dates
        # Calculate tenure
        # Add is_active flag
        # Return cleaned dataframe
    
    def transform_timesheets(self, ts_df):
        # Remove duplicates
        # Parse datetimes
        # Calculate late/early minutes
        # Flag anomalies
        # Return enriched dataframe
    
    def load_silver(self, emp_df, ts_df):
        # Register dataframes
        # Create SILVER tables
        # Unregister views
    
    def generate_kpis(self):
        # Execute 9 KPI queries
        # Create GOLD tables
        # Log results
    
    def run(self):
        # Execute full pipeline
        # Error handling
        # Close database connection
```

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| Language | Python | 3.11 |
| Data Processing | Pandas | 2.x |
| Database | DuckDB | Latest |
| API Framework | FastAPI | Latest |
| Visualization | Plotly | Latest |
| Environment | Virtual Environment (venv) | |
| Version Control | Git | |

---

## ğŸ“ Dependencies

See `requirements.txt`:

```
pandas>=2.0.0
duckdb>=0.8.0
plotly>=5.14.0
fastapi>=0.104.0
uvicorn>=0.24.0
python-dotenv>=1.0.0
```

---

## âš ï¸ Limitations & Future Improvements

### Current Limitations
1. No authentication for API endpoints
2. Data stored locally (not cloud-based)
3. Limited to single-threaded ETL execution
4. No scheduling mechanism for recurring runs

### Future Enhancements
1. **Scheduling:** Add Apache Airflow or Prefect for scheduled runs
2. **Cloud:** Migrate to cloud databases (Snowflake, BigQuery)
3. **Authentication:** Add JWT token-based API authentication
4. **Monitoring:** Implement data quality monitoring and alerting
5. **Real-time:** Stream processing for real-time KPIs
6. **ML:** Add predictive models for attrition forecasting

---

## ğŸ“‹ Testing Checklist

- [x] All CSVs located in `data/raw/`
- [x] Virtual environment created and activated
- [x] Dependencies installed via `pip install -r requirements.txt`
- [x] `.env` file created with proper configuration
- [x] ETL pipeline runs without errors
- [x] All 9 KPI tables created successfully
- [x] Database verification shows correct row counts
- [x] API server starts without errors
- [x] All 7 endpoints return valid JSON responses
- [x] Charts generated in `reports/` folder
- [x] Charts open correctly in browser

---

## ğŸ› Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'src'`
**Solution:** Run commands from project root directory with `python -m` prefix

### Issue: DuckDB connection error
**Solution:** Ensure `data/` directory exists:
```bash
mkdir -p data/{raw,bronze,silver,gold}
```

### Issue: API not responding
**Solution:** Check if server is running:
```bash
curl http://localhost:8000/health
```

### Issue: Charts not displaying
**Solution:** Use absolute paths or run from project root:
```bash
cd /path/to/data-onboarding-etl
python src/viz/charts.py
```

---

## ğŸ“š References & Resources

- [DuckDB Documentation](https://duckdb.org/)
- [Pandas Documentation](https://pandas.pydata.org/)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/)
- [Plotly Documentation](https://plotly.com/python/)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)

---

## âœ… Completion Checklist

- [x] ETL pipeline fully functional
- [x] All 3 data layers implemented (BRONZE, SILVER, GOLD)
- [x] 9 KPIs generated and validated
- [x] REST API with 7 endpoints
- [x] 6 interactive visualizations
- [x] Comprehensive README documentation
- [x] Source code properly organized
- [x] Error handling implemented
- [x] Logging enabled throughout

---

## ğŸ‘¨â€ğŸ’» Author

**Name:** Dikshant Bikram Thapa  
**University:** Kathmandu University  
**Program:** CSE, Final Year (8th Semester)  
**Email:** dikshant@email.com  

---

## ğŸ“œ License

This project is created for educational purposes as part of Data Engineering coursework.

---

## ğŸ™ Acknowledgments

- Kathmandu University for providing the assignment
- All open-source libraries and communities
- Classmates for feedback and support

---

**Last Updated:** January 6, 2026  
**Status:** âœ… Complete & Production Ready
